{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "EN2550 - Funadamentals of Image Processing and Machine Vision\r\n",
    "Assignment 04\r\n",
    "Name         : Rathnayaka R.G.H.V.\r\n",
    "Index Number : 180529E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing required libraries\r\n",
    "import cv2 as cv\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import tensorflow as tf\r\n",
    "from tensorflow import keras\r\n",
    "from tensorflow.keras import layers,models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.Linear Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.(a) Implementing gradient descent and running for 300 epochs\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # Importing data set CIFAR10\r\n",
    "K = len(np.unique(y_train)) # Classes (np.unique is to find the unique elements of an array)\r\n",
    "Ntr = x_train.shape[0] # Number of training data\r\n",
    "Nte = x_test.shape[0] # Number of testing data\r\n",
    "Din = x_train.shape[1]*x_train.shape[2]*x_train.shape[3] # CIFAR10\r\n",
    "    \r\n",
    "# Normalizing pixel values\r\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
    "    \r\n",
    "# Centering pixel values\r\n",
    "mean_image = np.mean(x_train, axis=0)\r\n",
    "x_train = x_train - mean_image\r\n",
    "x_test = x_test - mean_image\r\n",
    "\r\n",
    "# One hot encoding the labels\r\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\r\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\r\n",
    "\r\n",
    "# Flatterning the input images and changing the data type\r\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\r\n",
    "x_test = np.reshape(x_test,(Nte,Din))\r\n",
    "x_train = x_train.astype('float32')\r\n",
    "x_test = x_test.astype('float32')\r\n",
    "\r\n",
    "print('x_train:', x_train.shape)\r\n",
    "print('x_test:', x_test.shape)\r\n",
    "print('y_train:', y_train.shape)\r\n",
    "print('y_test:', y_test.shape)\r\n",
    "\r\n",
    "# Defining parameters\r\n",
    "iterations = 300 # Number of iterations in gradient descent\r\n",
    "lr = 1.4e-2 # Learning rate\r\n",
    "lr_decay= 0.999\r\n",
    "reg = 5e-6 # Regularization constant for the loss function - Lamda\r\n",
    "\r\n",
    "# Initializing weight and bias arrays\r\n",
    "Din=x_train.shape[1]\r\n",
    "std=1e-5\r\n",
    "w1 = std*np.random.randn(Din, K)\r\n",
    "b1 = np.zeros(K)\r\n",
    "K = y_test.shape[1]\r\n",
    "batch_size=x_train.shape[0]\r\n",
    "\r\n",
    "# Running the linear classifier\r\n",
    "lr_history = []\r\n",
    "loss_history = [] # Loss function values\r\n",
    "loss_history_test = [] \r\n",
    "train_acc_history = [] # Training accuracy\r\n",
    "val_acc_history = [] # Validating accuracy\r\n",
    "seed = 0\r\n",
    "rng = np.random.default_rng(seed=seed)\r\n",
    "\r\n",
    "# Defining a function for regularized loss\r\n",
    "def regloss(y_pred,y,w1,w2=0):\r\n",
    "    batch_size=y_pred.shape[0] # Determining the number of input data\r\n",
    "    loss=(1/(batch_size))*(np.square(y-y_pred)).sum()+reg*(np.sum(w1*w1)+np.sum(w2*w2))\r\n",
    "    return loss\r\n",
    "\r\n",
    "# Defining a function for accuracy\r\n",
    "def accuracy(y_pred,y):\r\n",
    "    batch_size=y_pred.shape[0] # Determining the number of input data\r\n",
    "    K=y_pred.shape[1] # Determining number of classes\r\n",
    "    acc=1-(1/(batch_size*K))*(np.abs(np.argmax(y,axis=1)-np.argmax(y_pred,axis=1))).sum()\r\n",
    "    return acc\r\n",
    "\r\n",
    "for t in range(iterations):\r\n",
    "    # Shuffling the training data set to randomize the training process.To prevent overfitting\r\n",
    "    indices = np.arange(Ntr)\r\n",
    "    rng.shuffle(indices)\r\n",
    "    x=x_train[indices]\r\n",
    "    y=y_train[indices]\r\n",
    "\r\n",
    "    # Forward pass\r\n",
    "    y_pred=x.dot(w1)+b1\r\n",
    "    y_pred_test=x_test.dot(w1)+b1\r\n",
    "\r\n",
    "    # Calculating losses\r\n",
    "    train_loss=regloss(y_pred,y,w1)\r\n",
    "    test_loss=regloss(y_pred_test,y_test,w1)\r\n",
    "    loss_history.append(train_loss)\r\n",
    "    loss_history_test.append(test_loss)\r\n",
    "        \r\n",
    "    # Calculating accuracies\r\n",
    "    train_acc=accuracy(y_pred,y)\r\n",
    "    train_acc_history.append(train_acc)\r\n",
    "    test_acc=accuracy(y_pred_test,y_test)\r\n",
    "    val_acc_history.append(test_acc)\r\n",
    "\r\n",
    "    if t%10 == 0:\r\n",
    "        print('epoch %d|%d:Learning rate= %f|Training loss= %f|Testing loss= %f|Training accuracy= %f|Testing accuracy= %f' % (t,iterations,lr,train_loss,test_loss,train_acc,test_acc))\r\n",
    "\r\n",
    "    # Backward pass\r\n",
    "    dy_pred=(1./batch_size)*2.0*(y_pred-y)\r\n",
    "    dw1=x.T.dot(dy_pred)+reg*w1\r\n",
    "    db1=dy_pred.sum(axis=0)\r\n",
    "\r\n",
    "    # Updating parameters\r\n",
    "    w1-=lr*dw1\r\n",
    "    b1-=lr*db1\r\n",
    "    # Decaying learning rate\r\n",
    "    lr_history.append(lr)\r\n",
    "    lr = lr*lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.(b) Showing the weights matrix W as 10 images\r\n",
    "images=[]\r\n",
    "titles=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\r\n",
    "for i in range(w1.shape[1]):\r\n",
    "    temp=np.reshape(w1[:,i]*255,(32,32,3))\r\n",
    "    temp=cv.normalize(temp, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)\r\n",
    "    images.append(temp)\r\n",
    "fig,ax=plt.subplots(2,5,figsize=(25,10))\r\n",
    "for i in range(2):\r\n",
    "    for j in range(5):\r\n",
    "        ax[i,j].imshow(images[i*5+j],vmin=0,vmax=255)\r\n",
    "        ax[i,j].set_xticks([])\r\n",
    "        ax[i,j].set_yticks([])\r\n",
    "        ax[i,j].set_title(titles[i*5+j])\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.(c) Plots for learning rate, training and testing loss and accuracies\r\n",
    "lists=[lr_history,loss_history,loss_history_test,train_acc_history,val_acc_history]\r\n",
    "titles=[\"Learning Rate\",\"Training loss\",\"Testing loss\",\"Training accuracy\",\"Testing accuracy\"]\r\n",
    "fig,ax=plt.subplots(1,5,figsize=(25,5))\r\n",
    "n=2\r\n",
    "labels=[]\r\n",
    "for i in range(len(lists)):\r\n",
    "    ax[i].plot(lists[i])\r\n",
    "    ax[i].set_ylabel(titles[i])\r\n",
    "    ax[i].set_xlabel(\"epoch\")\r\n",
    "    ax[i].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Two-Layer fully connected network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.(a) Implementing gradient descent and running for 300 epochs\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # Importing data set CIFAR10\r\n",
    "K = len(np.unique(y_train)) # Classes (np.unique is to find the unique elements of an array)\r\n",
    "Ntr = x_train.shape[0] # Number of training data\r\n",
    "Nte = x_test.shape[0] # Number of testing data\r\n",
    "Din = x_train.shape[1]*x_train.shape[2]*x_train.shape[3] # CIFAR10\r\n",
    "    \r\n",
    "# Centering pixel values\r\n",
    "mean_image = np.mean(x_train, axis=0)\r\n",
    "x_train = x_train - mean_image\r\n",
    "x_test = x_test - mean_image\r\n",
    "\r\n",
    "# One hot encoding the labels\r\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\r\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\r\n",
    "\r\n",
    "# Flatterning the input images and changing the data type\r\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\r\n",
    "x_test = np.reshape(x_test,(Nte,Din))\r\n",
    "x_train = x_train.astype('float32')\r\n",
    "x_test = x_test.astype('float32')\r\n",
    "\r\n",
    "print('x_train:', x_train.shape)\r\n",
    "print('x_test:', x_test.shape)\r\n",
    "print('y_train:', y_train.shape)\r\n",
    "print('y_test:', y_test.shape)\r\n",
    "\r\n",
    "# Defining parameters\r\n",
    "H=200 # Number of hidden nodes\r\n",
    "iterations = 300 # Number of iterations in gradient descent\r\n",
    "lr = 1.4e-2 # Learning rate\r\n",
    "lr_decay= 0.999\r\n",
    "reg = 5e-6 # Regularization constant for the loss function - Lamda\r\n",
    "\r\n",
    "# Initializing weight and bias arrays\r\n",
    "Din=x_train.shape[1]\r\n",
    "std=1e-5\r\n",
    "w1 = (2/(Ntr*Din))**0.5*np.random.randn(Din, H)\r\n",
    "w2 = (2/(H*Din))**0.5*np.random.randn(H, K)\r\n",
    "b1 = np.zeros(H)\r\n",
    "b2 = np.zeros(K)\r\n",
    "K = y_test.shape[1]\r\n",
    "batch_size=x_train.shape[0]\r\n",
    "\r\n",
    "# Running the linear classifier\r\n",
    "lr_history2 = []\r\n",
    "loss_history2 = [] # Loss function values\r\n",
    "loss_history_test2 = [] \r\n",
    "train_acc_history2 = [] # Training accuracy\r\n",
    "val_acc_history2 = [] # Validating accuracy\r\n",
    "seed = 0\r\n",
    "rng = np.random.default_rng(seed=seed)\r\n",
    "\r\n",
    "for t in range(iterations):\r\n",
    "    # Mini batching the training data set\r\n",
    "    indices = np.random.choice(Ntr,batch_size)\r\n",
    "    # Shuffling the training data set to avoid overfitting\r\n",
    "    rng.shuffle(indices)\r\n",
    "    x=x_train[indices]\r\n",
    "    y=y_train[indices]\r\n",
    "\r\n",
    "    # Forward pass\r\n",
    "    h=1/(1+np.exp(-(x.dot(w1)+b1)))\r\n",
    "    h_test=1/(1+np.exp(-((x_test).dot(w1)+b1)))\r\n",
    "    y_pred=h.dot(w2)+b2\r\n",
    "    y_pred_test=h_test.dot(w2)+b2\r\n",
    "\r\n",
    "    # Calculating the loss\r\n",
    "    train_loss=regloss(y_pred,y,w1,w2)\r\n",
    "    test_loss=regloss(y_pred_test,y_test,w1,w2)\r\n",
    "    loss_history2.append(train_loss)\r\n",
    "    loss_history_test2.append(test_loss)\r\n",
    "        \r\n",
    "    # Calculating accuracy\r\n",
    "    train_acc=accuracy(y_pred,y)\r\n",
    "    train_acc_history2.append(train_acc)\r\n",
    "    test_acc=accuracy(y_pred_test,y_test)\r\n",
    "    val_acc_history2.append(test_acc)\r\n",
    "\r\n",
    "    if t%10 == 0:\r\n",
    "        print('epoch %d|%d:Learning rate= %f|Training loss= %f|Testing loss= %f|Training accuracy= %f|Testing accuracy= %f' % (t,iterations,lr,train_loss,test_loss,train_acc,test_acc))\r\n",
    "\r\n",
    "    # Backward pass\r\n",
    "    dy_pred=(1./batch_size)*2.0*(y_pred-y)\r\n",
    "    dw2=h.T.dot(dy_pred)+reg*w2\r\n",
    "    db2=dy_pred.sum(axis=0)\r\n",
    "    dh=dy_pred.dot(w2.T)\r\n",
    "    dw1=x.T.dot(dh*h*(1-h))+reg*w1\r\n",
    "    db1=(dh*h*(1-h)).sum(axis=0)\r\n",
    "\r\n",
    "    # Updating parameters\r\n",
    "    w1-=lr*dw1\r\n",
    "    b1-=lr*db1\r\n",
    "    w2-=lr*dw2\r\n",
    "    b2-=lr*db2\r\n",
    "    # Decaying learning rate\r\n",
    "    lr_history2.append(lr)\r\n",
    "    lr = lr*lr_decay\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\r\n",
    "w12=(1/(1+np.exp(-w1))).dot(w2)\r\n",
    "titles=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\r\n",
    "for i in range(w12.shape[1]):\r\n",
    "    temp=np.reshape(w12[:,i]*255,(32,32,3))\r\n",
    "    temp=cv.normalize(temp, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)\r\n",
    "    images.append(temp)\r\n",
    "fig,ax=plt.subplots(2,5,figsize=(30,10))\r\n",
    "for i in range(2):\r\n",
    "    for j in range(5):\r\n",
    "        ax[i,j].imshow(images[i*5+j],vmin=0,vmax=255)\r\n",
    "        ax[i,j].set_xticks([])\r\n",
    "        ax[i,j].set_yticks([])\r\n",
    "        ax[i,j].set_title(titles[i*5+j])\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.(b) Plots for learning rate, training and testing loss and accuracies\r\n",
    "lists=[lr_history2,loss_history2,loss_history_test2,train_acc_history2,val_acc_history2]\r\n",
    "titles=[\"Learning Rate\",\"Training loss\",\"Testing loss\",\"Training accuracy\",\"Testing accuracy\"]\r\n",
    "fig,ax=plt.subplots(1,5,figsize=(25,5))\r\n",
    "n=2\r\n",
    "labels=[]\r\n",
    "for i in range(len(lists)):\r\n",
    "    ax[i].plot(lists[i])\r\n",
    "    ax[i].set_ylabel(titles[i])\r\n",
    "    ax[i].set_xlabel(\"epoch\")\r\n",
    "    ax[i].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Stochastic gradient descent with a batch size of 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.(a) Implementing gradient descent and running for 300 epochs\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # Importing data set CIFAR10\r\n",
    "K = len(np.unique(y_train)) # Classes (np.unique is to find the unique elements of an array)\r\n",
    "Ntr = x_train.shape[0] # Number of training data\r\n",
    "Nte = x_test.shape[0] # Number of testing data\r\n",
    "Din = x_train.shape[1]*x_train.shape[2]*x_train.shape[3] # CIFAR10\r\n",
    "    \r\n",
    "# Centering pixel values\r\n",
    "mean_image = np.mean(x_train, axis=0)\r\n",
    "x_train = x_train - mean_image\r\n",
    "x_test = x_test - mean_image\r\n",
    "\r\n",
    "# One hot encoding the labels\r\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\r\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\r\n",
    "\r\n",
    "# Flatterning the input images and changing the data type\r\n",
    "x_train = np.reshape(x_train,(Ntr,Din))\r\n",
    "x_test = np.reshape(x_test,(Nte,Din))\r\n",
    "x_train = x_train.astype('float32')\r\n",
    "x_test = x_test.astype('float32')\r\n",
    "\r\n",
    "print('x_train:', x_train.shape)\r\n",
    "print('x_test:', x_test.shape)\r\n",
    "print('y_train:', y_train.shape)\r\n",
    "print('y_test:', y_test.shape)\r\n",
    "\r\n",
    "# Defining parameters\r\n",
    "batch_size=500\r\n",
    "H=200 # Number of hidden nodes\r\n",
    "iterations = 300 # Number of iterations in gradient descent\r\n",
    "lr = 1.4e-2 # Learning rate\r\n",
    "lr_decay= 0.999\r\n",
    "reg = 5e-6 # Regularization constant for the loss function - Lamda\r\n",
    "\r\n",
    "# Initializing weight and bias arrays\r\n",
    "Din=x_train.shape[1]\r\n",
    "std=1e-5\r\n",
    "w1 = (2/(Ntr*Din))**0.5*np.random.randn(Din, H)\r\n",
    "w2 = (2/(H*Din))**0.5*np.random.randn(H, K)\r\n",
    "b1 = np.zeros(H)\r\n",
    "b2 = np.zeros(K)\r\n",
    "K = y_test.shape[1]\r\n",
    "\r\n",
    "# Running the linear classifier\r\n",
    "lr_history3 = []\r\n",
    "loss_history3 = [] # Loss function values\r\n",
    "loss_history_test3 = [] \r\n",
    "train_acc_history3 = [] # Training accuracy\r\n",
    "val_acc_history3 = [] # Validating accuracy\r\n",
    "seed = 0\r\n",
    "rng = np.random.default_rng(seed=seed)\r\n",
    "\r\n",
    "for t in range(iterations):\r\n",
    "    # Mini batching the training data set\r\n",
    "    indices = np.random.choice(Ntr,batch_size)\r\n",
    "    # Shuffling the training data set to avoid overfitting\r\n",
    "    rng.shuffle(indices)\r\n",
    "    x=x_train[indices]\r\n",
    "    y=y_train[indices]\r\n",
    "\r\n",
    "    # Forward pass\r\n",
    "    h=1/(1+np.exp(-(x.dot(w1)+b1)))\r\n",
    "    h_test=1/(1+np.exp(-((x_test).dot(w1)+b1)))\r\n",
    "    y_pred=h.dot(w2)+b2\r\n",
    "    y_pred_test=h_test.dot(w2)+b2\r\n",
    "\r\n",
    "    # Calculating the loss\r\n",
    "    train_loss=regloss(y_pred,y,w1,w2)\r\n",
    "    test_loss=regloss(y_pred_test,y_test,w1,w2)\r\n",
    "    loss_history3.append(train_loss)\r\n",
    "    loss_history_test3.append(test_loss)\r\n",
    "        \r\n",
    "    # Calculating accuracy\r\n",
    "    train_acc=accuracy(y_pred,y)\r\n",
    "    train_acc_history3.append(train_acc)\r\n",
    "    test_acc=accuracy(y_pred_test,y_test)\r\n",
    "    val_acc_history3.append(test_acc)\r\n",
    "\r\n",
    "    if t%10 == 0:\r\n",
    "        print('epoch %d|%d:Learning rate= %f|Training loss= %f|Testing loss= %f|Training accuracy= %f|Testing accuracy= %f' % (t,iterations,lr,train_loss,test_loss,train_acc,test_acc))\r\n",
    "\r\n",
    "    # Backward pass\r\n",
    "    dy_pred=(1./batch_size)*2.0*(y_pred-y)\r\n",
    "    dw2=h.T.dot(dy_pred)+reg*w2\r\n",
    "    db2=dy_pred.sum(axis=0)\r\n",
    "    dh=dy_pred.dot(w2.T)\r\n",
    "    dw1=x.T.dot(dh*h*(1-h))+reg*w1\r\n",
    "    db1=(dh*h*(1-h)).sum(axis=0)\r\n",
    "\r\n",
    "    # Updating parameters\r\n",
    "    w1-=lr*dw1\r\n",
    "    b1-=lr*db1\r\n",
    "    w2-=lr*dw2\r\n",
    "    b2-=lr*db2\r\n",
    "    # Decaying learning rate\r\n",
    "    lr_history3.append(lr)\r\n",
    "    lr = lr*lr_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images=[]\r\n",
    "w12=(1/(1+np.exp(-w1))).dot(w2)\r\n",
    "titles=['airplane','automobile','bird','cat','deer','dog','frog','horse','ship','truck']\r\n",
    "for i in range(w12.shape[1]):\r\n",
    "    temp=np.reshape(w12[:,i]*255,(32,32,3))\r\n",
    "    temp=cv.normalize(temp, None, 0, 255, cv.NORM_MINMAX, cv.CV_8U)\r\n",
    "    images.append(temp)\r\n",
    "fig,ax=plt.subplots(2,5,figsize=(30,10))\r\n",
    "for i in range(2):\r\n",
    "    for j in range(5):\r\n",
    "        ax[i,j].imshow(images[i*5+j],vmin=0,vmax=255)\r\n",
    "        ax[i,j].set_xticks([])\r\n",
    "        ax[i,j].set_yticks([])\r\n",
    "        ax[i,j].set_title(titles[i*5+j])\r\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.(b) Comparision of results\r\n",
    "lists=[[lr_history2,lr_history3],[loss_history2,loss_history3],[loss_history_test2,loss_history_test3],[train_acc_history2,train_acc_history3],[val_acc_history2,val_acc_history3]]\r\n",
    "titles=[\"Learning rate\",\"Training loss\",\"Testing loss\",\"Training accuracy\",\"Testing accuracy\"]\r\n",
    "fig,ax=plt.subplots(1,5,figsize=(25,5))\r\n",
    "n=2\r\n",
    "labels=[\"Without Mini Batching\",\"With Mini Batching\"]\r\n",
    "for i in range(len(lists)):\r\n",
    "    for j in range(n):\r\n",
    "        ax[i].plot(lists[i][j],label=labels[j])\r\n",
    "    ax[i].legend()  \r\n",
    "    ax[i].set_ylabel(titles[i])\r\n",
    "    ax[i].set_xlabel(\"epoch\")\r\n",
    "    ax[i].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.(a) Learnable parameters\r\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # Importing data set CIFAR10\r\n",
    "K = len(np.unique(y_train)) # Classes (np.unique is to find the unique elements of an array)\r\n",
    "Ntr = x_train.shape[0] # Number of training data\r\n",
    "Nte = x_test.shape[0] # Number of testing data\r\n",
    "Din = x_train.shape[1]*x_train.shape[2]*x_train.shape[3] # CIFAR10\r\n",
    "    \r\n",
    "# Normalizing pixel values\r\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\r\n",
    "    \r\n",
    "# Centering pixel values\r\n",
    "mean_image = np.mean(x_train, axis=0)\r\n",
    "x_train = x_train - mean_image\r\n",
    "x_test = x_test - mean_image\r\n",
    "\r\n",
    "# One hot encoding the labels\r\n",
    "y_train = tf.keras.utils.to_categorical(y_train, num_classes=K)\r\n",
    "y_test = tf.keras.utils.to_categorical(y_test, num_classes=K)\r\n",
    "\r\n",
    "x_train = x_train.astype('float32')\r\n",
    "x_test = x_test.astype('float32')\r\n",
    "\r\n",
    "print('x_train:', x_train.shape)\r\n",
    "print('x_test:', x_test.shape)\r\n",
    "print('y_train:', y_train.shape)\r\n",
    "print('y_test:', y_test.shape)\r\n",
    "\r\n",
    "model = models.Sequential()\r\n",
    "\r\n",
    "model.add(layers.Conv2D(32,(3,3),activation='relu',input_shape=(32,32,3)))\r\n",
    "model.add(layers.MaxPool2D((2,2)))\r\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\r\n",
    "model.add(layers.MaxPool2D((2,2)))\r\n",
    "model.add(layers.Conv2D(64,(3,3),activation='relu'))\r\n",
    "model.add(layers.MaxPool2D((2,2)))\r\n",
    "model.add(layers.Flatten())\r\n",
    "model.add(layers.Dense(64,activation='relu'))\r\n",
    "model.add(layers.Dense(10))\r\n",
    "\r\n",
    "model.summary()\r\n",
    "sgd = tf.keras.optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\r\n",
    "model.compile(optimizer=sgd,loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),metrics=['accuracy'])\r\n",
    "history=model.fit(x_train,y_train,epochs=45,batch_size=50,validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#4.(b) Learning rate & Momentum\r\n",
    "print(model.optimizer.get_config())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.(c) Plots for training and testing loss and accuracies\r\n",
    "lists=history.history['loss'],history.history['val_loss'],history.history['accuracy'],history.history['val_accuracy']\r\n",
    "titiles=[\"Training loss\",\"Testing loss\",\"Training accuracy\",\"Testing accuracy\"]\r\n",
    "fig,ax=plt.subplots(1,4,figsize=(25,5))\r\n",
    "n=2\r\n",
    "labels=[]\r\n",
    "for i in range(len(lists)):\r\n",
    "    ax[i].plot(lists[i])\r\n",
    "    ax[i].set_ylabel(titiles[i])\r\n",
    "    ax[i].set_xlabel(\"epoch\")\r\n",
    "    ax[i].plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit (conda)",
   "name": "python374jvsc74a57bd03073a49219fb3afe5764de15e36ed149cc46b76dbb8cd330f66fd11cec46daef"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}